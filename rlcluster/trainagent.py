import click
from rlcluster.agents.dqn import DQN
from rlcluster.agents.ddpg import DDPG
from rlcluster.agents.td3 import TD3
from rlcluster.agents.sac import SAC
from rlcluster.agents.vpg import VPG
from rlcluster.agents.ppo import PPO
netparameters = {'hidden_activation': 'leakyrelu'}

@click.group()
def rlclustercli(*args, **kwargs):
    """ Collection of Reinforcement learning algorithms """
    pass

@rlclustercli.command()
@click.option('--envid', type=str,  default='CartPole-v0', help='Environment Name')
@click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
@click.option('--savepath', type=str,  default='./results/', help='Path for saving information and models')
@click.option('--seed', type=int,  default=0, help='Seed number for random generators')
@click.option('--num_iterations', type=int, default=100, help='Number of Iterations')
@click.option('--num_evaluations', type=int, default=10, help='Number of episodes for Evaluating Model periodically')
@click.option('--evaluate_interval', type=int, default=50, help='Periodic Interval for Evaluating Model')
@click.option('--saving_interval', type=int, default=50, help='Periodic Interval for Saving Model')
@click.option('--warmup_steps', type=int, default=0, help='Number of timesteps for Exploration only')
@click.option('--minsteps_per_iteration', type=int, default=1000, help='Number of minimum environment Interactions in one-iteration')
@click.option('--buffer_size', type=int, default=100000, help='Size of Replay Buffer')
@click.option('--batch_size', type=int, default=100, help='Sampling Batchsize in Network updations')
@click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
@click.option('--epsilon', type=float, default=0.90, help='Epsilon factor for random exploration')
@click.option('--polyak_coefficient', type=float, default=0.995, help='Polyak averaging factor for Target-network parameters')
@click.option('--update_interval', type=int, default=1, help='Periodic Interval for allowing Network updation (update_interval times)')
@click.option('--update_target_interval', type=int, default=50, help='Periodic Interval for allowing target Network updation')
@click.option('--critic_lrq', type=float, default=1e-4, help='Learning rate for State-Action Value Network')
@click.option('--dim_filter', type=int, default=64, help='Number of filters of each convolution layer in Network')
@click.option('--dim_hidden', type=int, default=64, help='Number of hidden neurons of each layer in Network')
@click.option('--debug', type=bool, default=False, help='Debug mode for logging')
def dqn(**kwargs):
    ''' Deep Q-Network '''
    kwargs.update(netparameters)
    kwargs.update({"actor_lrp":None, 'output_layer':None})
    agent = DQN(kwargs['envid'], kwargs['render'], kwargs['seed'], kwargs, kwargs, kwargs['savepath'], kwargs['debug'])
    agent.learn(kwargs['num_iterations'], kwargs['num_evaluations'], kwargs['evaluate_interval'], kwargs['saving_interval'])

@rlclustercli.command()
@click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
@click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
@click.option('--savepath', type=str,  default='./results/', help='Path for saving information and models')
@click.option('--seed', type=int,  default=0, help='Seed number for random generators')
@click.option('--num_iterations', type=int, default=100, help='Number of Iterations')
@click.option('--num_evaluations', type=int, default=10, help='Number of episodes for Evaluating Model periodically')
@click.option('--evaluate_interval', type=int, default=50, help='Periodic Interval for Evaluating Model')
@click.option('--saving_interval', type=int, default=50, help='Periodic Interval for Saving Model')
@click.option('--warmup_steps', type=int, default=0, help='Number of timesteps for Exploration only')
@click.option('--minsteps_per_iteration', type=int, default=1000, help='Number of minimum environment Interactions in one-iteration')
@click.option('--buffer_size', type=int, default=50000, help='Size of Replay Buffer')
@click.option('--batch_size', type=int, default=100, help='Sampling Batchsize in Network updations')
@click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
@click.option('--polyak_coefficient', type=float, default=0.005, help='Polyak averaging factor for Target-network parameters')
@click.option('--action_noise_scale', type=float, default=0.1, help='Standard deviation of Gaussian Noise for actionspace exploration')
@click.option('--update_interval', type=int, default=50, help='Periodic Interval for allowing Network updation (update_interval times)')
@click.option('--actor_lrp', type=float, default=1e-4, help='Learning rate for Policy Network')
@click.option('--critic_lrq', type=float, default=1e-3, help='Learning rate for State-Action Value Network')
@click.option('--dim_filter', type=int, default=64, help='Number of filters of each convolution layer in Network')
@click.option('--dim_hidden', type=int, default=64, help='Number of hidden neurons of each layer in Network')
@click.option('--output_layer', type=str, default='tanh', help='Name of Output layer function')
@click.option('--debug', type=bool, default=False, help='Debug mode for logging')
def ddpg(**kwargs):
    ''' Deep Deterministic Policy Gradient '''
    kwargs.update(netparameters)
    agent = DDPG(kwargs['envid'], kwargs['render'], kwargs['seed'], kwargs, kwargs, kwargs['savepath'], kwargs['debug'])
    agent.learn(kwargs['num_iterations'], kwargs['num_evaluations'], kwargs['evaluate_interval'], kwargs['saving_interval'])

@rlclustercli.command()
@click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
@click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
@click.option('--savepath', type=str,  default='./results/', help='Path for saving information and models')
@click.option('--seed', type=int,  default=0, help='Seed number for random generators')
@click.option('--num_iterations', type=int, default=100, help='Number of Iterations')
@click.option('--num_evaluations', type=int, default=10, help='Number of episodes for Evaluating Model periodically')
@click.option('--evaluate_interval', type=int, default=50, help='Periodic Interval for Evaluating Model')
@click.option('--saving_interval', type=int, default=50, help='Periodic Interval for Saving Model')
@click.option('--warmup_steps', type=int, default=0, help='Number of timesteps for Exploration only')
@click.option('--minsteps_per_iteration', type=int, default=1000, help='Number of minimum environment Interactions in one-iteration')
@click.option('--buffer_size', type=int, default=50000, help='Size of Replay Buffer')
@click.option('--batch_size', type=int, default=100, help='Sampling Batchsize in Network updations')
@click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
@click.option('--polyak_coefficient', type=float, default=0.005, help='Polyak averaging factor for Target-network parameters')
@click.option('--action_noise_scale', type=float, default=0.1, help='Standard deviation of Gaussian Noise for actionspace exploration')
@click.option('--target_action_noise_scale', type=float, default=0.2, help='Standard deviation of Gaussian Noise for target-network action')
@click.option('--target_action_noise_clip', type=float, default=0.5, help='Clipping parameter of Noise for target-network action')
@click.option('--actor_update_delay', type=int, default=2, help='Periodic delay of Policy updation w.r.t Critic updation')
@click.option('--update_interval', type=int, default=50, help='Periodic Interval for allowing Network updation (update_interval times)')
@click.option('--actor_lrp', type=float, default=1e-3, help='Learning rate for Policy Network')
@click.option('--critic_lrq', type=float, default=1e-3, help='Learning rate for State-Action Value Network')
@click.option('--dim_filter', type=int, default=64, help='Number of filters of each convolution layer in Network')
@click.option('--dim_hidden', type=int, default=64, help='Number of hidden neurons of each layer in Network')
@click.option('--output_layer', type=str, default='tanh', help='Name of Output layer function')
@click.option('--debug', type=bool, default=False, help='Debug mode for logging')
def td3(**kwargs):
    ''' Twin-Delayed Deep Deterministic Policy Gradient '''
    kwargs.update(netparameters)
    agent = TD3(kwargs['envid'], kwargs['render'], kwargs['seed'], kwargs, kwargs, kwargs['savepath'], kwargs['debug'])
    agent.learn(kwargs['num_iterations'], kwargs['num_evaluations'], kwargs['evaluate_interval'], kwargs['saving_interval'])


@rlclustercli.command()
@click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
@click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
@click.option('--savepath', type=str,  default='./results/', help='Path for saving information and models')
@click.option('--seed', type=int,  default=1, help='Seed number for random generators')
@click.option('--num_iterations', type=int, default=100, help='Number of Iterations')
@click.option('--num_evaluations', type=int, default=10, help='Number of episodes for Evaluating Model periodically')
@click.option('--evaluate_interval', type=int, default=50, help='Periodic Interval for Evaluating Model')
@click.option('--saving_interval', type=int, default=50, help='Periodic Interval for Saving Model')
@click.option('--warmup_steps', type=int, default=0, help='Number of timesteps for Exploration only')
@click.option('--minsteps_per_iteration', type=int, default=1000, help='Number of minimum environment Interactions in one-iteration')
@click.option('--buffer_size', type=int, default=100000, help='Size of Replay Buffer')
@click.option('--batch_size', type=int, default=100, help='Sampling Batchsize in Network updations')
@click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
@click.option('--polyak_coefficient', type=float, default=0.005, help='Polyak averaging factor for Target-network parameters')
@click.option('--update_interval', type=int, default=50, help='Periodic Interval for allowing Network updation (update_interval times)')
@click.option('--alpha_lr', type=float, default=3e-4, help='Learning rate for alpha parameter')
@click.option('--actor_lrp', type=float, default=1e-3, help='Learning rate for Policy Network')
@click.option('--critic_lrq', type=float, default=1e-3, help='Learning rate for State-Action Value Network')
@click.option('--dim_filter', type=int, default=64, help='Number of filters of each convolution layer in Network')
@click.option('--dim_hidden', type=int, default=64, help='Number of hidden neurons of each layer in Network')
@click.option('--output_layer', type=str, default='beta', help='Name of Output activation function')
@click.option('--debug', type=bool, default=False, help='Debug mode for logging')
def sac(**kwargs):
    ''' Soft Actor-Critic Agent '''
    kwargs.update(netparameters)
    agent = SAC(kwargs['envid'], kwargs['render'], kwargs['seed'], kwargs, kwargs, kwargs['savepath'], kwargs['debug'])
    agent.learn(kwargs['num_iterations'], kwargs['num_evaluations'], kwargs['evaluate_interval'], kwargs['saving_interval'])


# @rlclustercli.command()
# @click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
# @click.option('--model_path', type=str,  default='./results/models/', help='Path for saving models')
# @click.option('--log_path', type=str,  default='./results/logs/', help='Path for saving logs')
# @click.option('--metadata_path', type=str,  default='./results/metadata/', help='Path for saving metadata')
# @click.option('--num_process', type=int,  default=1, help='Number of processes for Environment Interaction')
# @click.option('--seed', type=int,  default=1, help='Seed number for random generators')
# @click.option('--totaliters', type=int, default=100, help='Total number of Iterations')
# @click.option('--evaleps', type=int, default=10, help='Number of episodes for Evaluating Model')
# @click.option('--evalgap', type=int, default=50, help='Periodic Interval for Evaluating Model')
# @click.option('--savegap', type=int, default=50, help='Periodic Interval for Saving Model')
# @click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
# @click.option('--lr_ac', type=float, default=3e-4, help='Learning rate for Actor-Critic Network')
# @click.option('--valueloss_coeff', type=float, default=1.0, help='Scaling coefficient for ValueNet loss')
# @click.option('--entropy_coeff', type=float, default=0.0, help='Scaling coefficient for Entropy')
# @click.option('--gradientclip', type=float, default=40, help='Gradient clipping parameter for Network weights')
# @click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
# @click.option('--gaelambda', type=float, default=0.95, help='Lambda parameter for Generalized Advantage Estimation')
# @click.option('--distribution', type=str, default='Beta', help='ClassName of Stochastic distribution')
# @click.option('--batch_size', type=int, default=1024, help='Batchsize in Network updations / Size of Experience Collection')
# def a2c(**kwargs):
#     agent = A2C(kwargs['envid'], kwargs['render'], kwargs['lr_ac'], kwargs['valueloss_coeff'], kwargs['entropy_coeff'], kwargs['gradientclip'], 
#                 kwargs['gamma'], kwargs['gaelambda'], kwargs['distribution'], kwargs['batch_size'], 
#                 kwargs['model_path'], kwargs['log_path'], kwargs['metadata_path'], kwargs['num_process'], kwargs['seed'])
#     agent.learn(kwargs['totaliters'], kwargs['evaleps'], kwargs['evalgap'], kwargs['savegap'])


@rlclustercli.command()
@click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
@click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
@click.option('--savepath', type=str,  default='./results/', help='Path for saving information and models')
@click.option('--seed', type=int,  default=0, help='Seed number for random generators')
@click.option('--num_iterations', type=int, default=100, help='Number of Iterations')
@click.option('--num_evaluations', type=int, default=10, help='Number of episodes for Evaluating Model periodically')
@click.option('--evaluate_interval', type=int, default=50, help='Periodic Interval for Evaluating Model')
@click.option('--saving_interval', type=int, default=50, help='Periodic Interval for Saving Model')
@click.option('--maxsteps_per_iteration', type=int, default=1000, help='Number of Environment Interactions in one-iteration')
@click.option('--maxsteps_per_episode', type=int, default=1000, help='Number of Environment Interactions in one-episode')
@click.option('--actor_iterations', type=int, default=1, help='Maximum number of policy updates in each iteration')
@click.option('--critic_iterations', type=int, default=1, help='Maximum number of value updates in each iteration')
@click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
@click.option('--gaelambda', type=float, default=0.95, help='Lambda parameter for Generalized Advantage Estimation')
@click.option('--max_gradnorm', type=float, default=10.0, help='Max. gradient norm for Policy Network Parameters')
@click.option('--l2_regularizer', type=float, default=0.001, help='L2-regularizer coefficient Value Network Parameters')
@click.option('--actor_lr', type=float, default=3e-4, help='Learning rate for Policy Network')
@click.option('--critic_lr', type=float, default=3e-4, help='Learning rate for Value Network')
@click.option('--output_layer', type=str, default='beta', help='Name of Stochastic output distribution')
@click.option('--dim_filter', type=int, default=64, help='Number of filters of each convolution layer in Network')
@click.option('--dim_hidden', type=int, default=64, help='Number of hidden neurons of each layer in Network')
@click.option('--debug', type=bool, default=False, help='Debug mode for logging')
def vpg(**kwargs):
    ''' Vanilla Policy Gradient (Reinforce) '''
    kwargs.update(netparameters)
    agent = VPG(kwargs['envid'], kwargs['render'], kwargs['seed'], kwargs, kwargs, kwargs['savepath'], kwargs['debug'])
    agent.learn(kwargs['num_iterations'], kwargs['num_evaluations'], kwargs['evaluate_interval'], kwargs['saving_interval'])

@rlclustercli.command()
@click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
@click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
@click.option('--savepath', type=str,  default='./results/', help='Path for saving information and models')
@click.option('--seed', type=int,  default=0, help='Seed number for random generators')
@click.option('--num_iterations', type=int, default=100, help='Number of Iterations')
@click.option('--num_evaluations', type=int, default=10, help='Number of episodes for Evaluating Model periodically')
@click.option('--evaluate_interval', type=int, default=50, help='Periodic Interval for Evaluating Model')
@click.option('--saving_interval', type=int, default=50, help='Periodic Interval for Saving Model')
@click.option('--maxsteps_per_iteration', type=int, default=2000, help='Number of Environment Interactions in one-iteration')
@click.option('--maxsteps_per_episode', type=int, default=1000, help='Number of Environment Interactions in one-episode')
@click.option('--entropy_coefficient', type=float, default=0.0, help='Scaling factor for output distribution entropy')
@click.option('--clip_epsilon', type=float, default=0.2, help='Clipping Epsilon parameter for Policy update')
@click.option('--target_kldiv', type=float, default=None, help='Maximum limit for kl divergence between sucessive policies')
@click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
@click.option('--gaelambda', type=float, default=0.95, help='Lambda parameter for Generalized Advantage Estimation')
@click.option('--batch_size', type=int, default=64, help='Batchsize used in each epoch in each iteration')
@click.option('--num_epochs', type=int, default=10, help='Maximum number of epoches in each iteration')
@click.option('--max_gradnorm', type=float, default=10.0, help='Max. gradient norm for Policy Network Parameters')
@click.option('--l2_regularizer', type=float, default=0.001, help='L2-regularizer coefficient Value Network Parameters')
@click.option('--actor_lr', type=float, default=3e-4, help='Learning rate for Policy Network')
@click.option('--critic_lr', type=float, default=3e-4, help='Learning rate for Value Network')
@click.option('--output_layer', type=str, default='beta', help='Name of Stochastic output distribution')
@click.option('--dim_filter', type=int, default=64, help='Number of filters of each convolution layer in Network')
@click.option('--dim_hidden', type=int, default=64, help='Number of hidden neurons of each layer in Network')
@click.option('--debug', type=bool, default=False, help='Debug mode for logging')
def ppo(**kwargs):
    ''' Proximal Policy Optimization '''
    kwargs.update(netparameters)
    agent = PPO(kwargs['envid'], kwargs['render'], kwargs['seed'], kwargs, kwargs, kwargs['savepath'], kwargs['debug'])
    agent.learn(kwargs['num_iterations'], kwargs['num_evaluations'], kwargs['evaluate_interval'], kwargs['saving_interval'])

# @rlclustercli.command()
# @click.option('--envid', type=str,  default='Pendulum-v0', help='Environment Name')
# @click.option('--model_path', type=str,  default='./results/models/', help='Path for saving models')
# @click.option('--log_path', type=str,  default='./results/logs/', help='Path for saving logs')
# @click.option('--metadata_path', type=str,  default='./results/metadata/', help='Path for saving metadata')
# @click.option('--num_process', type=int,  default=1, help='Number of processes for Environment Interaction')
# @click.option('--seed', type=int,  default=1, help='Seed number for random generators')
# @click.option('--totaliters', type=int, default=100, help='Total number of Iterations')
# @click.option('--evaleps', type=int, default=10, help='Number of episodes for Evaluating Model')
# @click.option('--evalgap', type=int, default=50, help='Periodic Interval for Evaluating Model')
# @click.option('--savegap', type=int, default=50, help='Periodic Interval for Saving Model')
# @click.option('--render', type=bool, default=False, help='Render Environment during Evaluation')
# @click.option('--lr_v', type=float, default=3e-4, help='Learning rate for Value Network')
# @click.option('--v_regularizer', type=float, default=1e-3, help='L2 regularizer parameter for Value Network')
# @click.option('--use_lbfgs', type=bool, default=False, help='Use Lbfgs optimization / Gradient descent for Value Network')
# @click.option('--gamma', type=float, default=0.99, help='Discount factor for Reward')
# @click.option('--gaelambda', type=float, default=0.95, help='Lambda parameter for Generalized Advantage Estimation')
# @click.option('--max_kl', type=float, default=1e-2, help='Maximum kl-divergence between successive policies')
# @click.option('--cg_damping', type=float, default=1e-2, help='damping factor for Conjugate gradient method')
# @click.option('--distribution', type=str, default='Beta', help='ClassName of Stochastic distribution')
# @click.option('--batch_size', type=int, default=1024, help='Batchsize in Network updations / Size of Experience Collection')
# def trpo(**kwargs):
#     agent = TRPO(kwargs['envid'], kwargs['render'], kwargs['lr_v'], kwargs['v_regularizer'],  kwargs['use_lbfgs'], 
#                  kwargs['gamma'], kwargs['gaelambda'], kwargs['max_kl'], kwargs['cg_damping'], kwargs['distribution'], kwargs['batch_size'], 
#                  kwargs['model_path'], kwargs['log_path'], kwargs['metadata_path'], kwargs['num_process'], kwargs['seed'])
#     agent.learn(kwargs['totaliters'], kwargs['evaleps'], kwargs['evalgap'], kwargs['savegap'])
